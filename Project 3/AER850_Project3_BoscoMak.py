# -*- coding: utf-8 -*-
"""AER850 project 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cA0FfxB6o-eA5oxXlsiRJZEXh5WfEORW
"""

# install libraries

!pip install tensorflow-gpu
!pip install ultralytics opencv-python pillow matplotlib -q

from google.colab import drive
import os
import glob
import cv2
import numpy as np
import pathlib as Path
from ultralytics import YOLO
import matplotlib.pyplot as plt
from PIL import Image

drive.mount('/content/drive')
root = "/content/drive/MyDrive/Project 3 Data"
motherboardimg_dir = os.path.join(root,"motherboard_image.JPEG")
YAML_dir = os.path.join(root, "data/data.yaml")

eval_dir = os.path.join(root, "data/evaluation")
test_dir = os.path.join(root, "data/test")
train_dir = os.path.join(root, "data/train")
valid_dir = os.path.join(root, "data/valid")

outputimg_dir = os.path.join(root, "outputs")
os.makedirs(outputimg_dir, exist_ok=True)

def show_bgr_image(img_bgr, title="Image"):
    """Display a BGR (OpenCV) image with matplotlib."""
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(6, 6))
    plt.imshow(img_rgb)
    plt.title(title)
    plt.axis("off")
    plt.show()

print(os.listdir('/content/drive/MyDrive/Project 3 Data/'))

def show_bgr_image(img_bgr, title="Image"):
    """Display a BGR (OpenCV) image with matplotlib."""
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(6, 6))
    plt.imshow(img_rgb)
    plt.title(title)
    plt.axis("off")
    plt.show()

# ============================
# 4. STEP 1 – OBJECT MASKING
# ============================

# ---- 4.1 Load image ----
orig = cv2.imread(motherboardimg_dir)
if orig is None:
    raise FileNotFoundError(f"Could not read {motherboardimg_dir}. Check the path.")

show_bgr_image(orig, "Original Motherboard Image")

# ---- 4.2 Convert to grayscale & blur ----
gray = cv2.cvtColor(orig, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(gray, (5, 5), 0)


# ---- 4.3 Edge detection (Canny) ----

thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 51, 5)
edges = cv2.Canny(blur, threshold1=50, threshold2=150)

plt.figure(figsize=(6, 6))
plt.imshow(edges, cmap="gray")
plt.title("Adaptive Threshold")
plt.axis("off")
plt.show()

edges = thresh.copy()

# ---- 4.4 Find contours & keep the largest one (assumed PCB) ----
contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

if not contours:
    raise RuntimeError("No contours found – adjust Canny thresholds or preprocessing.")

largest_contour = max(contours, key=cv2.contourArea)

# Visualize contour on a copy
contour_img = orig.copy()
cv2.drawContours(contour_img, [largest_contour], -1, (0, 0, 255), 3)
show_bgr_image(contour_img, "Largest Contour (PCB Boundary)")

# ---- 4.5 Create a mask from the contour ----
mask = np.zeros(gray.shape, dtype=np.uint8)
cv2.drawContours(mask, [largest_contour], -1, 255, thickness=-1)  # filled contour

plt.figure(figsize=(6, 6))
plt.imshow(mask, cmap="gray")
plt.title("Mask (PCB Region)")
plt.axis("off")
plt.show()

# ---- 4.6 Extract PCB from background using bitwise_and ----
pcb_extracted = cv2.bitwise_and(orig, orig, mask=mask)
show_bgr_image(pcb_extracted, "Extracted PCB")

# ---- 4.7 Optional: Crop to bounding box for a tighter image ----
x, y, w, h = cv2.boundingRect(largest_contour)
pcb_cropped = pcb_extracted[y:y+h, x:x+w]
show_bgr_image(pcb_cropped, "Cropped PCB")

# ---- 4.8 Save images for report ----
cv2.imwrite(os.path.join(outputimg_dir, "edges.png"), edges)
cv2.imwrite(os.path.join(outputimg_dir, "mask.png"), mask)
cv2.imwrite(os.path.join(outputimg_dir, "pcb_extracted.png"), pcb_extracted)
cv2.imwrite(os.path.join(outputimg_dir, "pcb_cropped.png"), pcb_cropped)

"""
Assumptions:
- You have YOLOv11 weights available by default in ultralytics (e.g., 'yolo11n.pt').
- Your dataset at DATA_YAML_PATH is in YOLO format:
  - data.yaml specifies 'train', 'val', 'nc', and 'names'.
"""

# ---- 5.1 Load pretrained YOLOv11 nano model ----
# If the exact name is different (e.g., 'yolo11n.pt' vs 'yolo11n'), update the string.
model = YOLO("yolo11n.pt")  # pretrained model

# ---- 5.2 Train configuration (change as needed) ----
EPOCHS = 100      # must remain below 200 as per instructions
BATCH_SIZE = 8   # adjust depending on VRAM
IMG_SIZE = 960    # >= 900 as per instructions

results = model.train(
    data=YAML_dir,
    epochs=EPOCHS,
    batch=BATCH_SIZE,
    imgsz=IMG_SIZE,
    name="pcb_yolo11n",      # run name (creates runs/detect/pcb_yolo11n)
    project=os.path.join(root, "runs"),  # put runs inside your project folder
    exist_ok=True
)

# After training, ultralytics will save:
# - best.pt in /runs/detect/pcb_yolo11n/weights
# - training curves (PR curve, P-confidence, confusion matrix, etc.) in the same folder

# ============================
# 6. STEP 3 – EVALUATION ON 3 IMAGES
# ============================

# ---- 6.1 Load best trained weights (optional; if you just trained above, `model` is already trained) ----
# If you've trained previously and are restarting the notebook, uncomment these lines and set the correct path:
# BEST_WEIGHTS_PATH = os.path.join(PROJECT_ROOT, "runs", "detect", "pcb_yolo11n", "weights", "best.pt")
# model = YOLO(BEST_WEIGHTS_PATH)

# ---- 6.2 Get evaluation image paths ----
eval_image_paths = [
    os.path.join(eval_dir, fname)
    for fname in os.listdir(eval_dir)
    if fname.lower().endswith((".jpg", ".jpeg", ".png"))
]

print("Evaluation images:")
for p in eval_image_paths:
    print(" -", p)

# ---- 6.3 Run prediction on each evaluation image ----
EVAL_OUTPUT_DIR = os.path.join(root, "eval_outputs")
os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)

for img_path in eval_image_paths:
    print(f"\nRunning detection on: {img_path}")
    preds = model.predict(
        source=img_path,
        imgsz=IMG_SIZE,
        conf=0.25,        # adjust confidence threshold if needed
        save=True,        # saves images with boxes
        project=EVAL_OUTPUT_DIR,
        name="pcb_eval",
        exist_ok=True
    )

    # Display the first result inline
    result = preds[0]
    # result.plot() returns an array (BGR)
    plotted = result.plot()
    show_bgr_image(plotted, title=f"Detections: {os.path.basename(img_path)}")

    # For your report, you can inspect result.boxes to see what was detected
    print("Detected classes (indices):", result.boxes.cls.cpu().numpy())
    print("Confidence scores:", result.boxes.conf.cpu().numpy())

print("\nEvaluation complete. Check the 'eval_outputs/pcb_eval' folder in Drive for saved images.")